# Chat Notes - 2026-01-15

## Topics Covered

1. [ManifestProcessor Class Explanation](#1-manifestprocessor-class-explanation)
2. [Lambda Manifest Builder Deep Dive](#2-lambda-manifest-builder-deep-dive)
3. [Event Sequence with Multiple Lambda Instances](#3-event-sequence-with-multiple-lambda-instances)
4. [Configuration Updates (200 files per manifest)](#4-configuration-updates)
5. [File Size Validation Discussion](#5-file-size-validation)
6. [Lambda State Manager Modularity](#6-lambda-state-manager-modularity)

---

## 1. ManifestProcessor Class Explanation

**File:** `environments/prod/glue/glue_batch_job.py:42`

The `ManifestProcessor` class is the core of the Glue ETL job.

### Purpose
- Processes manifest files containing batches of NDJSON files
- Converts NDJSON to Parquet format
- Optimizes Spark for high-throughput processing

### Key Methods

| Method | Purpose |
|--------|---------|
| `__init__()` | Initialize with Spark, buckets, compression |
| `_configure_spark()` | Optimize Spark settings (adaptive execution, S3 fast upload) |
| `process_manifest()` | Main entry point - download manifest, extract files |
| `_process_manifest_content()` | Read NDJSON, cast to string, write Parquet |
| `_read_and_merge_ndjson()` | Read files, add metadata columns |
| `_cast_all_to_string()` | Convert all columns to string (schema consistency) |
| `_write_parquet()` | Write with intelligent partitioning (~128MB per file) |

### Spark Optimizations
```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")
spark.conf.set("spark.hadoop.fs.s3a.fast.upload", "true")
spark.conf.set("spark.hadoop.fs.s3a.multipart.size", "104857600")  # 100MB
```

---

## 2. Lambda Manifest Builder Deep Dive

**File:** `environments/prod/lambda/lambda_manifest_builder.py`

### Processing Flow

```
S3 Upload → SQS (batch_size=10) → Lambda Manifest Builder
                                        │
                    ┌───────────────────┼───────────────────┐
                    ▼                   ▼                   ▼
              Validate File      Track in DynamoDB    Check Batch Threshold
                    │                   │                   │
                    ▼                   ▼                   ▼
              Quarantine if      Status: "pending"    Create Manifest if
              invalid                                 >= 700 MB accumulated
```

### Key Components

**DistributedLock (lines 49-97)**
- Prevents race conditions when multiple Lambdas run
- Uses DynamoDB conditional writes
- TTL-based expiration (300 seconds)

**File Validation (lines 167-179)**
```python
# Current config: accepts 0.1 MB - 20 MB
min_size = EXPECTED_FILE_SIZE_MB * (1 - SIZE_TOLERANCE_PERCENT / 100)
max_size = EXPECTED_FILE_SIZE_MB * (1 + SIZE_TOLERANCE_PERCENT / 100)
```

**Date Extraction (lines 198-206)**
```python
# Regex finds date anywhere in path
# Example: "folder1/2025-12-23-file001.ndjson" → date: "2025-12-23"
match = re.search(r'(\d{4}-\d{2}-\d{2})', key)
```

**Batch Creation (lines 296-317)**
- Groups files until MAX_BATCH_SIZE_GB reached
- Keeps partial batches if >50% full

---

## 3. Event Sequence with Multiple Lambda Instances

### Scenario: 200 files uploaded, SQS batch_size=10

| Time | Lambda | DynamoDB Files | Total Size | Action |
|------|--------|----------------|------------|--------|
| T=0ms | #1 invoked | 0 | 0 MB | SQS polls 10 messages |
| T=500ms | #1 processing | 10 | 35 MB | Track files, check threshold |
| T=1000ms | #2 invoked | 10 | 35 MB | SQS polls next 10 messages |
| ... | ... | ... | ... | ... |
| T=9500ms | #20 processing | 200 | 700 MB | **Threshold reached!** |

### Manifest Creation Trigger
```python
total_size_bytes = 200 * 3.5 * 1024 * 1024  # 700 MB
threshold = 0.7 * 1024 * 1024 * 1024         # 700 MB

if total_size_bytes >= threshold:
    # Create manifest with all 200 files!
```

### Lock Coordination
- Only ONE Lambda creates the manifest (acquires lock first)
- Other Lambdas skip manifest creation if lock held
- Lock released after manifest created

---

## 4. Configuration Updates

### Changed From → To

| Setting | Old Value | New Value | Reason |
|---------|-----------|-----------|--------|
| `max_files_per_manifest` | 100 | **200** | 30% Glue cost savings |
| `max_batch_size_gb` | 0.35 | **0.7** | 200 files × 3.5 MB |
| `expected_file_size_mb` | 4096 | **10** | Actual files are 3-4 MB |
| `size_tolerance_percent` | 10 | **99** | Flexible for future variation |

### Performance Comparison

| Metric | 100 files | 200 files |
|--------|-----------|-----------|
| Manifest size | 350 MB | 700 MB |
| Glue cost | Baseline | **30% savings** |
| Latency to first Parquet | 5 min | 7 min (+2 min) |
| Parquet output files | 1-2 | 2-3 |

### Files Modified
- `terraform/terraform.tfvars` (dev)
- `terraform/terraform.tfvars.prod` (prod)

**No code changes required** - Lambda reads from environment variables.

---

## 5. File Size Validation

### Why Validation Exists
- Catch corrupted/truncated uploads
- Prevent wrong file types from entering pipeline
- Data quality gate

### Problem with Strict Validation
- Only 1 month of historical data
- File sizes may vary in future
- Strict range (2.8-4.2 MB) would quarantine valid files

### Solution: Permissive Validation

```python
# New settings
expected_file_size_mb  = 10
size_tolerance_percent = 99

# Resulting range
min_size = 10 * (1 - 99/100) = 0.1 MB  (100 KB)
max_size = 10 * (1 + 99/100) = 19.9 MB (~20 MB)
```

**Accepted:** 0.1 MB - 20 MB (covers your 3-4 MB files + future variation)

**Rejected:**
- Files < 100 KB (likely corrupted/empty)
- Files > 20 MB (likely wrong file type)

---

## 6. Lambda State Manager Modularity

**File:** `environments/prod/lambda/lambda_state_manager.py`

### What It Is
Admin/operations utility Lambda (NOT part of main pipeline flow)

### How It's Triggered

| Trigger | Frequency | Operation |
|---------|-----------|-----------|
| EventBridge | Every 6 hours | `find_orphans` |
| EventBridge | Every 1 hour | `get_stats` |
| Manual (CLI/Console) | On-demand | Any operation |

**Note:** Schedules only enabled in production (`enable_schedules = true`)

### Operations (All Independent)

| Operation | Category | Safe to Remove? |
|-----------|----------|-----------------|
| `get_file_state` | Query | Yes |
| `query_by_status` | Query | Yes |
| `get_stats` | Metrics | Yes |
| `get_processing_timeline` | Query | Yes |
| `find_orphans` | Maintenance | Yes |
| `reset_stuck_files` | Maintenance | Yes |
| `validate_consistency` | Maintenance | Yes |
| `update_state` | Admin | Yes |
| `batch_update` | Admin | Yes |

### Key Point: Fully Decoupled
- Pipeline works WITHOUT this Lambda
- Can add/remove functions without breaking anything
- Purely for monitoring and maintenance

### To Remove Metrics (Example)
```hcl
# Option 1: Disable schedules (Terraform)
enable_schedules = false

# Option 2: Remove get_stats function (Code)
# Delete lines 418-450 from lambda_state_manager.py
```

---

## Quick Reference

### File Locations
```
environments/prod/lambda/
├── lambda_manifest_builder.py  # File validation, tracking, batching
└── lambda_state_manager.py     # Admin/ops utility

environments/prod/glue/
└── glue_batch_job.py           # NDJSON → Parquet conversion

terraform/
├── terraform.tfvars            # Dev config
└── terraform.tfvars.prod       # Prod config
```

### Current Config Summary
```hcl
max_files_per_manifest = 200    # Files per batch
max_batch_size_gb      = 0.7    # 700 MB threshold
expected_file_size_mb  = 10     # Validation center
size_tolerance_percent = 99     # Accepts 0.1 - 20 MB
```

### DynamoDB File Status Flow
```
pending → manifested → processing → completed
                                  → failed
              ↓
          quarantined (if validation fails)
```
