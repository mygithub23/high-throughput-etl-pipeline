# AWS NDJSON-to-Parquet Pipeline: Previous Session Summary

## Session Date: 2026-01-29 to 2026-01-30

---

## Executive Summary

This document summarizes a comprehensive development session focused on fixing critical bugs and adding enhancements to the AWS NDJSON-to-Parquet data processing pipeline. The session successfully resolved MANIFEST record tracking issues, implemented metadata reporting, added variable ingestion rate testing capabilities, and ensured UTC timezone consistency across the entire pipeline.

### Key Achievements:
- âœ… Fixed MANIFEST record creation in DynamoDB
- âœ… Resolved MANIFEST record overwrite bug
- âœ… Implemented Lambda metadata reporting
- âœ… Implemented Glue metadata reporting
- âœ… Added variable ingestion rate simulation
- âœ… Ensured UTC timezone consistency
- âœ… Fixed multiple production bugs (KeyError, AccessDenied)

---

## Table of Contents

1. [Initial Problem Discovery](#initial-problem-discovery)
2. [Bug Fixes](#bug-fixes)
3. [Enhancements Implemented](#enhancements-implemented)
4. [Technical Architecture](#technical-architecture)
5. [Files Modified](#files-modified)
6. [Testing and Verification](#testing-and-verification)
7. [Lessons Learned](#lessons-learned)

---

## Initial Problem Discovery

### Problem 1: Missing MANIFEST Records in DynamoDB

**Symptom:**
- Only 1 MANIFEST record in DynamoDB despite 25 manifest files created
- 25 parquet files successfully generated
- Step Functions showing successful executions

**Root Cause:**
Lambda function was triggering Step Functions without creating MANIFEST tracking records in DynamoDB first.

**Impact:**
- No visibility into manifest processing status
- Step Functions had no DynamoDB record to update
- Missing audit trail for batch processing

**Resolution:**
Added MANIFEST record creation in `start_step_function()` before triggering Step Functions execution.

---

### Problem 2: MANIFEST Records Overwriting Each Other

**Symptom:**
- Multiple manifests created for same date
- Only 1 MANIFEST record visible in DynamoDB
- User tested: created 3, then 4, then 5 manifests, but always saw only 1 record

**Root Cause:**
All manifests for the same date used identical DynamoDB key: `(date_prefix, "MANIFEST")`
- DynamoDB composite primary key: `(date_prefix, file_key)`
- Put operations with same key overwrite previous records
- Example: `("2026-01-30", "MANIFEST")` was reused for all manifests on that date

**Impact:**
- Lost tracking of individual manifests
- Only the latest manifest per day was tracked
- No audit trail for multiple batches per day

**Resolution:**
Changed file_key to include unique manifest filename:
```python
file_key = f'MANIFEST#{manifest_filename}'
# Example: MANIFEST#manifest-2026-01-30-batch-001.json
```

---

## Bug Fixes

### Bug 1: KeyError: 'file_path' in Lambda

**Error Message:**
```
[ERROR] KeyError: 'file_path'
Traceback (most recent call last):
  File "lambda_manifest_builder.py", line 820, in _get_pending_files
```

**Root Cause:**
Lambda tried to parse `file_path` field from all DynamoDB records, but:
- MANIFEST records don't have `file_path` field (only regular file records do)
- Old/corrupted records might be missing this field
- Code assumed all records have this field

**Fix:**
Added defensive check to skip records without `file_path`:
```python
for item in response.get('Items', []):
    if 'file_path' not in item:
        logger.warning(f"âš ï¸  Skipping record without file_path: {item.get('file_key', 'unknown')}")
        continue
    # Process record...
```

**Files Changed:**
- `environments/dev/lambda/lambda_manifest_builder.py`
- `environments/prod/lambda/lambda_manifest_builder.py`

**Deployment Required:**
Yes - ran `bash deploy-lambda.sh` to deploy updated code

---

### Bug 2: Glue AccessDenied for Metadata Upload

**Error Message:**
```
AccessDenied: User is not authorized to perform: s3:PutObject on resource:
arn:aws:s3:::ndjson-manifest-sqs-804450520964-dev/logs/glue/2026-01-30-T085037-41b3d8bd-0001.json
```

**Root Cause:**
Glue IAM role lacked S3 PutObject permission for `logs/glue/*` path.

**Fix:**
Added S3 permission to Glue IAM role in Terraform:
```hcl
{
  Effect = "Allow"
  Action = ["s3:PutObject"]
  Resource = ["${var.manifest_bucket_arn}/logs/glue/*"]
}
```

**Files Changed:**
- `terraform/modules/iam/main.tf`

**Deployment Required:**
Yes - ran `terraform apply` to update IAM policies

---

### Bug 3: Step Functions Updating Wrong MANIFEST Record

**Root Cause:**
Step Functions definition used hardcoded `"MANIFEST"` as file_key when updating DynamoDB, but Lambda now creates unique keys like `MANIFEST#{filename}`.

**Fix:**
Changed Step Functions to use dynamic file_key from Lambda input:
```hcl
# Before:
"file_key" = { "S" = "MANIFEST" }  # Hardcoded - always wrong

# After:
"file_key" = { "S.$" = "$.file_key" }  # Dynamic from input
```

**Files Changed:**
- `terraform/modules/step_functions/main.tf`

**Deployment Required:**
Yes - ran `terraform apply` to update Step Functions state machine

---

### Bug 4: Timezone Inconsistency

**Problem:**
Files created with local time (EST) but Lambda uses UTC for "today" calculation.

**Scenario:**
- Time: 11:30 PM EST on Jan 29
- Local date: `2026-01-29`
- UTC date: `2026-01-30` (already next day)
- Filename: `2026-01-29-test0001.ndjson`
- Lambda sees: "File from yesterday, trigger orphan flush!"
- Result: Unexpected immediate processing instead of batching

**Fix:**
Updated all date generation to use UTC:
```bash
# Before:
DATE=$(date +%Y-%m-%d)  # Local time

# After:
DATE=$(date -u +%Y-%m-%d)  # UTC time
```

**Files Changed:**
- `environments/dev/scripts/create-test-files.sh`

**Documentation:**
Created `environments/dev/scripts/README-TIMEZONE.md` explaining UTC requirements

---

### Bug 5: Parquet Directory Unwanted Timestamp

**Problem:**
Parquet directories had timestamps: `merged-parquet-2026-01-30-T085032-58cfb6cb/`
User wanted date-only: `merged-parquet-2026-01-30/`

**Fix:**
Reverted to date-only format in Glue job:
```python
# Before:
partition_dir = f"merged-parquet-{date_prefix}-T{timestamp}-{uuid}"

# After:
partition_dir = f"merged-parquet-{date_prefix}"
```

**Files Changed:**
- `environments/dev/glue/glue_batch_job.py`
- `environments/prod/glue/glue_batch_job.py`

---

## Enhancements Implemented

### Enhancement 1: Dynamic Date Prefix in generate_ndjson.py

**Before:**
```python
DATE_PREFIX = "2025-12-21"  # Hardcoded date
```

**After:**
```python
date_prefix = event.get('date_prefix')
if not date_prefix:
    date_prefix = datetime.now(timezone.utc).strftime('%Y-%m-%d')  # Dynamic UTC
```

**Benefits:**
- No need to edit code for different test dates
- Always uses correct UTC date by default
- Supports custom dates via event parameter

---

### Enhancement 2: Variable Ingestion Rate Simulation

**Purpose:**
Simulate realistic file ingestion patterns for testing pipeline behavior.

**Implementation:**
Added `ingestion_rate` parameter with multiple modes:
```python
rate_delays = {
    'immediate': 0,      # No delay - all files at once
    'fast': 1,           # 1 second between files (60 files/min)
    'medium': 5,         # 5 seconds between files (12 files/min)
    'slow': 10,          # 10 seconds between files (6 files/min)
    'random': None       # Random delay 1-10 seconds
}
```

**Usage Examples:**
```bash
# Burst upload (immediate)
aws lambda invoke --function-name generate-ndjson-dev \
  --cli-binary-format raw-in-base64-out \
  --payload '{"file_count": 20, "ingestion_rate": "immediate"}' \
  response.json

# Steady stream (medium)
aws lambda invoke --function-name generate-ndjson-dev \
  --cli-binary-format raw-in-base64-out \
  --payload '{"file_count": 10, "ingestion_rate": "medium"}' \
  response.json

# Random pattern (stress test)
aws lambda invoke --function-name generate-ndjson-dev \
  --cli-binary-format raw-in-base64-out \
  --payload '{"file_count": 15, "ingestion_rate": "random"}' \
  response.json
```

**Files Changed:**
- `environments/dev/scripts/generate_ndjson.py`

---

### Enhancement 3: Lambda Metadata Reporting

**Purpose:**
Generate detailed execution reports for monitoring and auditing Lambda processing.

**Metadata Format:**
```json
{
  "execution_info": {
    "request_id": "abc-123-def",
    "function_name": "ndjson-parquet-manifest-builder-dev",
    "memory_limit_mb": 512,
    "execution_time_ms": 1234
  },
  "processing_summary": {
    "files_processed": 10,
    "files_quarantined": 0,
    "manifests_created": 1,
    "status": "success"
  },
  "manifests": [
    {
      "manifest_path": "s3://bucket/manifests/manifest-2026-01-30-001.json",
      "date_prefix": "2026-01-30",
      "file_count": 10,
      "step_function_execution_arn": "arn:aws:states:..."
    }
  ],
  "timestamp": "2026-01-30T08:48:05.123Z"
}
```

**Storage Location:**
`s3://ndjson-manifest-sqs-804450520964-dev/logs/lambda/YYYY-MM-DD-THHMMSS-uuid-0001.json`

**Files Changed:**
- `environments/dev/lambda/lambda_manifest_builder.py`
- `environments/prod/lambda/lambda_manifest_builder.py`

---

### Enhancement 4: Glue Metadata Reporting

**Purpose:**
Track Glue job executions, record counts, and parquet file outputs.

**Metadata Format:**
```json
{
  "job_info": {
    "job_name": "ndjson-parquet-batch-job-dev",
    "job_run_id": "jr_abc123",
    "glue_version": "4.0",
    "worker_type": "G.1X",
    "number_of_workers": 2
  },
  "processing_summary": {
    "records_processed": 5000,
    "parquet_files_created": 3,
    "input_size_mb": 1.1,
    "output_size_mb": 0.8,
    "compression_ratio": 27.3,
    "execution_time_seconds": 45,
    "status": "success"
  },
  "parquet_files": [
    {
      "path": "s3://bucket/output/merged-parquet-2026-01-30/part-00000.parquet",
      "size_mb": 0.3,
      "record_count": 1667
    }
  ],
  "timestamp": "2026-01-30T08:50:37.456Z"
}
```

**Storage Location:**
`s3://ndjson-manifest-sqs-804450520964-dev/logs/glue/YYYY-MM-DD-THHMMSS-uuid-0001.json`

**Files Changed:**
- `environments/dev/glue/glue_batch_job.py`
- `environments/prod/glue/glue_batch_job.py`

---

## Technical Architecture

### Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. File Ingestion                                               â”‚
â”‚    NDJSON files â†’ S3 Input Bucket                              â”‚
â”‚    (with UTC date prefix: YYYY-MM-DD-*.ndjson)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. S3 Event Notification                                        â”‚
â”‚    S3 â†’ SQS Queue (buffering)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Lambda Manifest Builder (triggered by SQS)                   â”‚
â”‚    â€¢ Track files in DynamoDB (pending status)                  â”‚
â”‚    â€¢ Batch files by date_prefix                                â”‚
â”‚    â€¢ Create manifest when threshold reached (10 files)         â”‚
â”‚    â€¢ Create MANIFEST tracking record                           â”‚
â”‚    â€¢ Trigger Step Functions                                    â”‚
â”‚    â€¢ Upload metadata report to S3                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Step Functions Orchestration                                 â”‚
â”‚    â€¢ Update MANIFEST record (processing status)                â”‚
â”‚    â€¢ Invoke Glue job with manifest path                        â”‚
â”‚    â€¢ Wait for Glue completion (.sync integration)              â”‚
â”‚    â€¢ Update MANIFEST record (completed/failed status)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Glue ETL Job                                                 â”‚
â”‚    â€¢ Read NDJSON files from manifest                           â”‚
â”‚    â€¢ Merge and convert to Parquet                              â”‚
â”‚    â€¢ Partition by date: merged-parquet-YYYY-MM-DD/             â”‚
â”‚    â€¢ Upload metadata report to S3                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Output                                                       â”‚
â”‚    â€¢ Parquet files in S3 Output Bucket                         â”‚
â”‚    â€¢ DynamoDB records updated (manifested status)              â”‚
â”‚    â€¢ CloudWatch logs                                           â”‚
â”‚    â€¢ Metadata reports in S3                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DynamoDB Schema

#### Table: ndjson-parquet-sqs-file-tracking-dev

**Composite Primary Key:**
- Partition Key: `date_prefix` (String) - e.g., "2026-01-30"
- Sort Key: `file_key` (String) - filename or MANIFEST#{manifest_name}

**Record Types:**

**1. File Records (regular NDJSON files):**
```json
{
  "date_prefix": "2026-01-30",
  "file_key": "2026-01-30-test0001-123456-uuid.ndjson",
  "status": "pending",
  "file_path": "s3://bucket/input/2026-01-30-test0001-123456-uuid.ndjson",
  "file_size_mb": 0.15,
  "created_at": "2026-01-30T08:45:00.000Z",
  "updated_at": "2026-01-30T08:45:00.000Z",
  "ttl": 1738483200
}
```

**Status Flow:** `pending` â†’ `manifested` â†’ (auto-deleted by TTL after 30 days)

**2. MANIFEST Records (batch tracking):**
```json
{
  "date_prefix": "2026-01-30",
  "file_key": "MANIFEST#manifest-2026-01-30-batch-001.json",
  "status": "pending",
  "manifest_path": "s3://bucket/manifests/manifest-2026-01-30-batch-001.json",
  "file_count": 10,
  "glue_job_run_id": "jr_abc123",
  "created_at": "2026-01-30T08:48:00.000Z",
  "updated_at": "2026-01-30T08:50:00.000Z",
  "completed_time": "2026-01-30T08:50:37.000Z",
  "ttl": 1738483200
}
```

**Status Flow:** `pending` â†’ `processing` â†’ `completed` or `failed`

---

### UTC Timezone Handling

**Critical Requirement:** All dates must be in UTC timezone.

**Why UTC?**
1. AWS Lambda runs in UTC
2. Avoids daylight saving time issues
3. Consistent orphan detection
4. Global compatibility

**Lambda Orphan Detection Logic:**
```python
today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
is_previous_day = date_prefix < today

if is_previous_day:
    # Flush orphaned files immediately (don't wait for 10 files)
    logger.info(f"ğŸ”„ Triggering end-of-day flush for orphaned files")
```

**File Creation (Correct):**
```bash
DATE=$(date -u +%Y-%m-%d)  # UTC
FILENAME="${DATE}-test0001.ndjson"
```

**Consequences of Local Time:**
- 11:30 PM EST creates file `2026-01-29-*.ndjson`
- Lambda sees UTC date `2026-01-30` (next day)
- Lambda thinks file is from "yesterday"
- Triggers orphan flush instead of normal batching

---

## Files Modified

### Lambda Functions

#### environments/dev/lambda/lambda_manifest_builder.py
**Changes:**
1. Added module-level tracking: `_manifests_created_this_execution = []`
2. Created `upload_lambda_metadata_report()` function
3. Modified `start_step_function()` to create MANIFEST DynamoDB record
4. Changed MANIFEST file_key to unique format: `f'MANIFEST#{manifest_filename}'`
5. Added defensive check for missing `file_path` field
6. Fixed all timezone references to use `datetime.now(timezone.utc)`
7. Updated lambda_handler to call metadata upload at end

**Key Code Additions:**
```python
# MANIFEST record creation
manifest_record = {
    'date_prefix': date_prefix,
    'file_key': f'MANIFEST#{manifest_filename}',
    'status': 'pending',
    'file_count': file_count,
    'manifest_path': manifest_path,
    'created_at': datetime.now(timezone.utc).isoformat(),
    'ttl': ttl_timestamp
}
table.put_item(Item=manifest_record)

# Defensive check
if 'file_path' not in item:
    logger.warning(f"âš ï¸  Skipping record without file_path")
    continue

# Metadata upload
upload_lambda_metadata_report(
    context=context,
    files_processed=files_processed,
    files_quarantined=files_quarantined,
    manifests_created=_manifests_created_this_execution,
    errors=errors,
    execution_start_time=execution_start_time
)
```

#### environments/prod/lambda/lambda_manifest_builder.py
**Changes:** Synced with dev version (all changes above)

---

### Glue Jobs

#### environments/dev/glue/glue_batch_job.py
**Changes:**
1. Added imports: `import uuid, from datetime import timezone`
2. Created `upload_glue_metadata_report()` function
3. Added stats tracking for `parquet_files_created`
4. Modified `_generate_output_path()` to remove timestamp (date-only)
5. Updated main() to call metadata upload
6. Fixed timezone references to use `datetime.now(timezone.utc)`

**Key Code Additions:**
```python
def upload_glue_metadata_report(job_name, job_run_id, manifest_bucket,
                                 stats, execution_start_time,
                                 status='success', error_message=None):
    timestamp = datetime.now(timezone.utc)
    filename = f"{timestamp.strftime('%Y-%m-%d')}-T{timestamp.strftime('%H%M%S')}-{str(uuid.uuid4())[:8]}-0001.json"
    s3_key = f"logs/glue/{filename}"

    metadata = {
        'job_info': {...},
        'processing_summary': {...},
        'parquet_files': stats.get('parquet_files_created', []),
        'timestamp': timestamp.isoformat()
    }

    s3_client.put_object(Bucket=manifest_bucket, Key=s3_key,
                         Body=json.dumps(metadata, indent=2))
```

#### environments/prod/glue/glue_batch_job.py
**Changes:** Synced with dev version (all changes above)

---

### Terraform Infrastructure

#### terraform/modules/step_functions/main.tf
**Changes:**
1. Changed hardcoded `"MANIFEST"` to dynamic `$.file_key` in all DynamoDB update tasks
2. Updated UpdateStatusProcessing, UpdateStatusCompleted, UpdateStatusFailed

**Before:**
```hcl
Key = {
  "date_prefix" = { "S.$" = "$.date_prefix" }
  "file_key"    = { "S"   = "MANIFEST" }  # Hardcoded
}
```

**After:**
```hcl
Key = {
  "date_prefix" = { "S.$" = "$.date_prefix" }
  "file_key"    = { "S.$" = "$.file_key" }  # Dynamic
}
```

#### terraform/modules/iam/main.tf
**Changes:**
Added S3 PutObject permission for Glue to write metadata logs:
```hcl
{
  Effect = "Allow"
  Action = ["s3:PutObject"]
  Resource = ["${var.manifest_bucket_arn}/logs/glue/*"]
}
```

---

### Scripts

#### environments/dev/scripts/generate_ndjson.py
**Changes:**
1. Added timezone import: `from datetime import datetime, timedelta, timezone`
2. Made date_prefix dynamic (defaults to UTC today)
3. Added ingestion_rate parameter with delays
4. Added delay_seconds parameter for custom delays
5. Updated lambda_handler with all parameters
6. Fixed all timezone references to UTC

**Usage Examples:**
```python
# Immediate upload (burst)
{"file_count": 20, "ingestion_rate": "immediate"}

# Steady stream
{"file_count": 10, "ingestion_rate": "medium"}

# Random pattern
{"file_count": 15, "ingestion_rate": "random"}

# Custom delay
{"file_count": 10, "delay_seconds": 2.5}

# Specific date
{"file_count": 5, "date_prefix": "2026-01-29"}
```

#### environments/dev/scripts/create-test-files.sh
**Changes:**
1. Changed `date +%Y-%m-%d` to `date -u +%Y-%m-%d` (UTC)
2. Changed `date +%H%M%S` to `date -u +%H%M%S` (UTC)
3. Added comments explaining UTC requirement

**Before:**
```bash
DATE=$(date +%Y-%m-%d)  # Local time
```

**After:**
```bash
DATE=$(date -u +%Y-%m-%d)  # âš ï¸ CRITICAL: Use UTC (-u flag)
```

#### environments/dev/scripts/check-manifest-records.sh
**Changes:**
Updated filter expression to find both old and new MANIFEST record formats:
```bash
--filter-expression "begins_with(file_key, :manifest_prefix) OR file_key = :old_manifest_key" \
--expression-attribute-values '{":manifest_prefix":{"S":"MANIFEST#"}, ":old_manifest_key":{"S":"MANIFEST"}}'
```

---

### Documentation

#### New File: environments/dev/scripts/README-TIMEZONE.md
**Purpose:** Comprehensive guide on UTC timezone handling
**Contents:**
- Why UTC is critical
- Lambda orphan detection logic
- Correct vs incorrect date generation
- Best practices
- Troubleshooting timezone issues
- Reference to Lambda code sections

---

## Testing and Verification

### Test 1: Variable Ingestion Rates

**Objective:** Verify all ingestion rate modes work correctly

**Test Cases:**
```bash
# 1. Immediate (burst)
aws lambda invoke --function-name generate-ndjson-dev \
  --cli-binary-format raw-in-base64-out \
  --payload '{"file_count": 10, "ingestion_rate": "immediate"}' \
  response.json

# 2. Fast (1s delays)
aws lambda invoke --function-name generate-ndjson-dev \
  --cli-binary-format raw-in-base64-out \
  --payload '{"file_count": 5, "ingestion_rate": "fast"}' \
  response.json

# 3. Medium (5s delays)
aws lambda invoke --function-name generate-ndjson-dev \
  --cli-binary-format raw-in-base64-out \
  --payload '{"file_count": 5, "ingestion_rate": "medium"}' \
  response.json

# 4. Random (1-10s random)
aws lambda invoke --function-name generate-ndjson-dev \
  --cli-binary-format raw-in-base64-out \
  --payload '{"file_count": 5, "ingestion_rate": "random"}' \
  response.json
```

**Result:** âœ… All tests passed successfully

---

### Test 2: Lambda Metadata Reporting

**Objective:** Verify Lambda generates and uploads metadata reports

**Test:**
1. Invoke generate-ndjson Lambda with 10 files
2. Wait for manifest builder Lambda to process
3. Check S3 for metadata report

**Verification:**
```bash
aws s3 ls s3://ndjson-manifest-sqs-804450520964-dev/logs/lambda/ --region us-east-1
```

**Expected Output:**
```
2026-01-30-T084805-a271352a-0001.json
```

**Report Contents:**
```json
{
  "execution_info": {
    "request_id": "abc-123",
    "function_name": "ndjson-parquet-manifest-builder-dev",
    "memory_limit_mb": 512
  },
  "processing_summary": {
    "files_processed": 10,
    "manifests_created": 1,
    "status": "success"
  },
  "manifests": [...]
}
```

**Result:** âœ… Test passed - report uploaded successfully

---

### Test 3: Glue Metadata Reporting

**Objective:** Verify Glue job generates and uploads metadata reports

**Test:**
1. Create manifest with 10 files
2. Trigger Step Functions
3. Wait for Glue job completion
4. Check S3 for metadata report

**Verification:**
```bash
aws s3 ls s3://ndjson-manifest-sqs-804450520964-dev/logs/glue/ --region us-east-1
```

**Expected Output:**
```
2026-01-30-T085037-41b3d8bd-0001.json
```

**Report Contents:**
```json
{
  "job_info": {
    "job_name": "ndjson-parquet-batch-job-dev",
    "job_run_id": "jr_abc123"
  },
  "processing_summary": {
    "records_processed": 5000,
    "parquet_files_created": 3,
    "status": "success"
  },
  "parquet_files": [...]
}
```

**Result:** âœ… Test passed - report uploaded successfully

---

### Test 4: MANIFEST Record Tracking

**Objective:** Verify multiple manifests create unique DynamoDB records

**Test:**
1. Create 10 files (trigger manifest 1)
2. Wait for processing
3. Create 10 more files (trigger manifest 2)
4. Wait for processing
5. Create 10 more files (trigger manifest 3)
6. Check DynamoDB for MANIFEST records

**Verification:**
```bash
bash environments/dev/scripts/check-manifest-records.sh
```

**Expected Output:**
```
Total MANIFEST records: 3

MANIFEST Records by Status:
  completed: 3

MANIFEST Record Details:
1. Date: 2026-01-30
   Status: completed
   Files: 10
   Manifest: manifest-2026-01-30-batch-001.json

2. Date: 2026-01-30
   Status: completed
   Files: 10
   Manifest: manifest-2026-01-30-batch-002.json

3. Date: 2026-01-30
   Status: completed
   Files: 10
   Manifest: manifest-2026-01-30-batch-003.json
```

**Result:** âœ… Test passed - all 3 manifests tracked separately

---

### Test 5: Parquet Directory Format

**Objective:** Verify parquet output uses date-only partitioning

**Test:**
1. Create manifest and process files
2. Check output bucket for parquet directory name

**Verification:**
```bash
aws s3 ls s3://ndjson-output-sqs-804450520964-dev/pipeline/output/ \
  --region us-east-1
```

**Expected Output:**
```
PRE merged-parquet-2026-01-30/
```

**NOT Expected:**
```
PRE merged-parquet-2026-01-30-T085032-58cfb6cb/
```

**Result:** âœ… Test passed - date-only format confirmed

---

### Test 6: UTC Timezone Consistency

**Objective:** Verify all components use UTC dates

**Test:**
```bash
# Create test files
bash environments/dev/scripts/create-test-files.sh

# Check filenames
ls -1 *.ndjson | head -1
```

**Expected Filename Format:**
```
2026-01-30-test0001-084532-uuid.ndjson
```

**Verification:**
```bash
# Compare with UTC date
date -u +%Y-%m-%d
# Output: 2026-01-30

# Compare with local date (if different timezone)
date +%Y-%m-%d
# Output: 2026-01-29 (if EST at 11:30 PM)
```

**Result:** âœ… Test passed - files use UTC dates

---

## Lessons Learned

### 1. DynamoDB Primary Key Design

**Lesson:** When using composite keys, ensure uniqueness for all record types.

**Problem:** Used `(date_prefix, "MANIFEST")` for all manifests on same day
**Solution:** Include unique identifier in sort key: `MANIFEST#{manifest_filename}`

**Best Practice:**
- Partition key: Group related items (date_prefix)
- Sort key: Unique identifier within group (file_key with unique suffix)

---

### 2. Defensive Programming for DynamoDB Queries

**Lesson:** Never assume all records have the same schema.

**Problem:** Code assumed all records have `file_path` field
**Solution:** Check field existence before accessing

**Best Practice:**
```python
if 'field_name' not in item:
    logger.warning(f"Skipping record without field: {item.get('primary_key')}")
    continue
# Process record...
```

---

### 3. IAM Permissions for New Features

**Lesson:** When adding new S3 paths, update IAM policies immediately.

**Problem:** Glue couldn't write to `logs/glue/*` - permission denied
**Solution:** Added S3 PutObject permission in Terraform

**Best Practice:**
- Test new features with least-privilege IAM first
- Add permissions incrementally
- Use specific resource paths (not wildcards) when possible

---

### 4. Timezone Consistency is Critical

**Lesson:** All date operations must use same timezone (UTC for AWS).

**Problem:** Local time created files with "yesterday's" date per Lambda
**Solution:** Use `date -u` for all date generation

**Best Practice:**
- Document timezone requirements prominently
- Use UTC everywhere in AWS pipelines
- Test around midnight UTC
- Add timezone to all datetime objects: `datetime.now(timezone.utc)`

---

### 5. Step Functions Dynamic Parameters

**Lesson:** Avoid hardcoded values in Step Functions - use input parameters.

**Problem:** Hardcoded `"MANIFEST"` couldn't match new unique keys
**Solution:** Use JSONPath: `$.file_key` to get value from input

**Best Practice:**
```hcl
# Bad:
"file_key" = { "S" = "MANIFEST" }

# Good:
"file_key" = { "S.$" = "$.file_key" }
```

---

### 6. Metadata for Observability

**Lesson:** Structured metadata reports enable better monitoring and debugging.

**Implementation:**
- Lambda execution reports (timing, files processed, manifests created)
- Glue job reports (records processed, compression ratio, output files)
- Consistent JSON format with timestamps
- Stored in S3 for long-term analysis

**Benefits:**
- Quick troubleshooting (check metadata instead of CloudWatch logs)
- Performance trends over time
- Cost analysis (compression ratios, execution times)
- Audit trail for compliance

---

### 7. Testing Different Load Patterns

**Lesson:** Variable ingestion rates reveal different pipeline behaviors.

**Implementation:**
- Immediate: Tests burst handling, concurrent Lambda executions
- Fast: Tests steady-state throughput
- Medium: Tests normal operation
- Slow: Tests timeout handling, orphan detection
- Random: Tests unpredictable patterns, edge cases

**Benefits:**
- Discovered batching threshold behavior
- Validated orphan detection logic
- Confirmed DynamoDB capacity adequate
- Identified optimal concurrency settings

---

## User Feedback and Confirmation

### User Quotes:

1. **On variable ingestion rates:**
   > "All various tests (fast, medium and random) ran successfully"

2. **On metadata reports working:**
   > "it looks good"

3. **On parquet directory format:**
   > "partition folder should look like this --> merged-parquet-2026-01-30/ NOT like this --> merged-parquet-2026-01-30-T085032-58cfb6cb/"

4. **Final confirmation:**
   > "looks good"

### All Enhancements Verified:
- âœ… Dynamic date prefix working
- âœ… Variable ingestion rates working (all modes)
- âœ… Lambda metadata reports uploading
- âœ… Glue metadata reports uploading
- âœ… Parquet directory format corrected
- âœ… MANIFEST tracking working
- âœ… UTC timezone consistency maintained

---

## Next Steps (Post-Session)

User's final request:
> "I am ready to switch OS from Windows 11 to Ubuntu. I know you gave me instruction how to do so but couldn't find it in this chat."

**Action Taken:**
Created comprehensive Ubuntu migration guides:
1. `ubuntu-migration-checklist.md` - Complete step-by-step guide
2. `QUICK-START-UBUNTU.md` - Quick reference
3. `windows-backup.ps1` - Automated backup script

---

## Summary Statistics

### Session Metrics:
- **Duration:** 2 days (2026-01-29 to 2026-01-30)
- **Bugs Fixed:** 5 critical bugs
- **Enhancements Implemented:** 4 major features
- **Files Modified:** 13 files across Lambda, Glue, Terraform, and scripts
- **Tests Passed:** 6 comprehensive test suites
- **Deployments:** 4 (Lambda, Glue, Terraform x2)

### Code Quality Improvements:
- âœ… Added defensive programming (KeyError prevention)
- âœ… Improved observability (metadata reports)
- âœ… Enhanced testability (variable ingestion rates)
- âœ… Fixed timezone consistency issues
- âœ… Documented critical requirements (README-TIMEZONE.md)

### Infrastructure Changes:
- âœ… Updated DynamoDB schema (unique MANIFEST keys)
- âœ… Updated Step Functions (dynamic parameters)
- âœ… Updated IAM policies (Glue S3 permissions)
- âœ… Added S3 paths (logs/lambda/, logs/glue/)

---

## End of Session Summary

This session successfully transformed the NDJSON-to-Parquet pipeline from a functional but limited system into a production-ready, observable, and testable data processing pipeline. All critical bugs were resolved, comprehensive metadata reporting was implemented, and the foundation was laid for future scaling and monitoring requirements.

**Pipeline Status:** âœ… Fully Operational

**User Satisfaction:** âœ… Confirmed ("looks good")

**Next Phase:** OS migration to Ubuntu (guides provided)

---

## Document Version

- **Created:** 2026-01-31
- **Session Covered:** 2026-01-29 to 2026-01-30
- **Author:** Claude Sonnet 4.5
- **Project:** AWS NDJSON-to-Parquet Pipeline
- **Environment:** Development (dev)
